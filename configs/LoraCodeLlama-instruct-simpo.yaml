# Model arguments
# model_name_or_path: ./output_dir/loraWeight/trace_CRFLP/checkpoint-14000
# torch_dtype: null
# use_flash_attention_2: true

# Data training arguments
# dataset_mixer:
#  princeton-nlp/llama3-ultrafeedback: 1.0
# dataset_splits:
# - train
# - test
#preprocessing_num_workers: 12

# SimPOTrainer arguments
#bf16: true
# beta: 2.5
# gamma: 1.4
# do_eval: true
# evaluation_strategy: steps
# eval_steps: 400
# gradient_accumulation_steps: 8
# gradient_checkpointing: true
# gradient_checkpointing_kwargs:
#   use_reentrant: False
# hub_model_id: simpo-exps
# learning_rate: 1.0e-6
# log_level: info
# logging_steps: 5
# lr_scheduler_type: cosine
# max_length: 2048
# max_prompt_length: 1800
# num_train_epochs: 1
# optim: adamw_torch
 output_dir: ./output_dir/loraWeight/trace_CRFLP/checkpoint-14000
# run_name: LoraCodeLLama-instruct-DPO
# per_device_train_batch_size: 1
# per_device_eval_batch_size: 2
# push_to_hub: false
# save_strategy: "steps"
# save_steps: 2000
# report_to:
# - wandb
# save_total_limit: 20
 seed: 42
# warmup_ratio: 0.1
